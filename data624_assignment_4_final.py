# -*- coding: utf-8 -*-
"""DATA624_Assignment_4_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lrzau6K5YkFsyzhcyIZyyeHvdcvujf8r

# Import Libraries
"""

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import seaborn as sns
pd.options.plotting.backend = "plotly"
import plotly.express as px
import matplotlib.pyplot as plt

pip install -U ydata-profiling

from ydata_profiling import ProfileReport
from IPython.display import HTML

from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer
from sklearn.metrics import silhouette_samples, silhouette_score, homogeneity_completeness_v_measure

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

import scipy.cluster.hierarchy as shc
from scipy.cluster.hierarchy import dendrogram, linkage

import itertools

"""# Load Data"""

df = pd.read_csv('DATA624_Assignment4_Data.csv')
df

"""# Exploratory Analysis"""

# Printed basic information and statistics about `df`
print(df.info())
print(df.describe())

"""From printing basic information about `df`, we can see that the dataframe consists of 6170 rows and 14 columns. There are no missing values in any of the columns. We can also see that the first 12 columns (i.e., columns 0 to 12) are floats whereas the last two columns (i.e., columns 13 and 14) are integers. The mean, standard deviation and the quartile values (i.e., min, 25%, 50%, 75%, max) of each variable are also displayed. From these, we could see that the scales of the float columns were approximately similar. """

# Generated a profile report for `df` using the `ProfileReport` class from the `ydata_profiling` module
profile = ProfileReport(
    df, 
    title="Nifty Profiling Report",
    # dark_mode=True,
    orange_mode=True,
    minimal=True,
)

## Displayed the profile report
display(HTML(profile.to_html()))

# ## Saved the profile report to a HTML report
# profile.to_file("nifty_profiling_report.html")

"""Though I experimented with using Google Facets Overview, Google Facets Dive and seaborn histograms to explore how the variables looked individually, I found that using the `ProfileReport` class from the `ydata_profiling` module allowed me to generate the most comprehensive, easy-to-follow summary of the characteristics of each variable. 

The generated Nifty Report (displayed above) contains detailed information and statistics about the data alongside a histogram displaying the distribution of each variable. Using the histogram for each column alongside our knowledge of the standard deviation of each column from earlier, we can see that all the float columns (i.e., columns from 0 to 12) except for columns 0, 5, 6, 7, 8 and 9 appear approximately normally distributed. Compared with the columns I considered to have 'approximately normal' distributions, columns 0 and 7 have relatively greater standard deviations (i.e., 2.509250 and 3.248121 respsectively). I considered the distributions for columns 6, 8 and 9 non-normal because they have much steeper domes compared to the column distributions I considered approximately normal. Finally, I considered columns 5 and 7 non-normally distributed because column 5 had a multimodal distribution with three peaks while column 7 had a bimodal distribution. 

The Nifty Report also revealed that the values in column 13 (named 3.1 in `df`) were all '0' while all the values in column 14 (named 4.1 in `df`) were '0' or '1'. We can also see that column 14 had more '0' values than '1' values. 

Because column 13 consists only of 0s and column 14 consists only of 0s or 1s, they are not well-suited for clustering algorithms designed for continuous variables. As such, for the rest of my analysis (including my clustering analysis), I decided to exclude the last two columns (i.e., the integer columns) of `df`.
"""

# Removed the last two columns of `df`
df = df.drop(['3.1', '4.1'], axis=1)

# Plotted pairwise relationships of `df`
sns.pairplot(df)
plt.title('Pairwise Relationships')
plt.show()

# Generated a heatmap the correlation matrix among the columns in `df`
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""From the pairplot and correlation matrix (displayed above), I wanted see whether there appeared to be relationships between pairs of variables and if so, what those relationships look like. Notably, there did not appear to be any moderate or strong linear correlations (neither positive nor negative) between any pairs of variables. According to the correlation matrix, the strongest linear relationship was the negative association (-0.31) between columns 6 and 9.

# Clustering Analysis

## KMeans

### Deciding on the number of clusters...

#### ...using the elbow method

First, I used the elbow method to (visually) decide on the number of clusters that I want to identify in my data. I used the **yellowbrick** package to generate my elbow plot (as  recommended by Rink (2022)).
"""

fig, ax = plt.subplots()

visualizer = KElbowVisualizer(KMeans(), k=(1,10),ax=ax)
visualizer.fit(df)

ax.set_xticks(range(1,10))
visualizer.show()
plt.show()

"""From the elbow plot above, we can see that the the slope of the line appears to level off when k = 3. As such, we can infer from the elbow method plot that the optimal number of clusters is 3.

#### ...using the silhouette method

I then used the silhouette method to confirm my conclusion from the elbow method (i.e., that k = 3 is the optimal number of clusters).
"""

sscore = []
for i in range(2,10):
    kmeans = KMeans(
        n_clusters = i, 
        init = "k-means++", 
        max_iter= 100, 
        tol=0.0001,
    ).fit(
        df
    )
    
    silhouette_avg = silhouette_score(df, kmeans.labels_)
    sscore.append(silhouette_avg)
    
px.line(y = sscore,x = range(2,10), markers=True,
        labels={'y': "Silhouette Score",
                'x': "Number of Clusters"})

"""From the silhouette score plot method plot above, we can see that highest silhouette score is for when k = 4. As such, we can infer from the silhouette score method plot that the optimal number of clusters is k = 4. This conflicts with the findings from the elbow score method plot where we inferred that the optimal number of clusters was k = 3.

#### ... using the silhouette analysis method
"""

# Performed comparative analysis to determine the optimal cluster number using silhouette plots and silhouette scores

fig, ax = plt.subplots(1, 2, figsize=(15,8))

for i in [3,4]:

    ## Created KMeans instance for different number of clusters
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)

    ## Created SilhouetteVisualizer instance with KMeans instance and fitted the visualizer
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[i-3])
    visualizer.fit(df)

    ## Calculated and displayed silhouette score
    score = silhouette_score(df, km.labels_)
    visualizer.ax.set_title(f"Silhouette Score: {score:.2f}")

"""Both charts showed the presence of clusters with below average silhouette scores (as indicated by the dotted red line on the charts) as well as fluctuations in the thickness of the bars in each chart. That said, from the charts, it is evident that the average silhouette score of the silhouette bar chart with 4 clusters is higher than the average silhouette score of the silhouette bar chart with 3 clusters. This suggests that 4 clusters is more optimal than 3 clusters.

### Visualizing the clusters...

Given that the elbow score method plot revealed that the optimal number of clusters was k = 3 while the silhouette score plot method and the silhouette analysis method revealed that the optimal number of clusters was k = 4, I decided to visualize the clusters when k = 3 and when k = 4.

#### ... using 3 clusters
"""

# Generated all possible combinations of column pairs
column_pairs = itertools.combinations(df.columns, 2)

# Define the number of rows and columns for the subplot matrix
nrows = 16
ncols = 5

# Created a figure and subplots with the defined number of rows and columns
fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 40))

# Reshaped the axis array to a flat array for easy indexing
ax = ax.flatten()

# Initialized list to store silhouette scores
silhouette_scores_k3 = []

# Looped over each column pair and plot the KMeans clustering results
for i, pair in enumerate(column_pairs):
    # Extract the two columns for this pair
    col1, col2 = pair
    data = df[[col1, col2]]
    
    # Performed KMeans clustering with 4 clusters
    kmeans_3 = KMeans(n_clusters=3, init='k-means++', max_iter=100, random_state=42)
    kmeans_3.fit(data)
    labels_k3 = kmeans_3.labels_

    # Calculated silhouette score and add to list of scores
    score_k3 = silhouette_score(data, labels_k3)
    silhouette_scores_k3.append(score_k3)
    
    # Visualized the clustering results on the subplot matrix
    ax[i].scatter(data[col1], data[col2], c=labels_k3, cmap='viridis')
    ax[i].set_xlabel(col1)
    ax[i].set_ylabel(col2)
    ax[i].set_title(f'KMeans Clustering (3 clusters) - {col1} vs. {col2}')

# Calculated average silhouette score
avg_silhouette_score_kmeans3 = sum(silhouette_scores_k3) / len(silhouette_scores_k3)

# Removed any extra subplots from the subplot matrix
for j in range(i+1, nrows*ncols):
    fig.delaxes(ax[j])

# Adjusted the spacing between the subplots and show the figure
fig.tight_layout()
plt.show()

# Printed average silhouette score
print(avg_silhouette_score_kmeans3)

"""#### ... using 4 clusters"""

# Generated all possible combinations of column pairs
column_pairs = itertools.combinations(df.columns, 2)

# Defined the number of rows and columns for the subplot matrix
nrows = 16
ncols = 5

# Created a figure and subplots with the defined number of rows and columns
fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 40))

# Reshaped the axis array to a flat array for easy indexing
ax = ax.flatten()

# Initialized list to store silhouette scores
silhouette_scores_k4 = []

# Looped over each column pair and plot the KMeans clustering results
for i, pair in enumerate(column_pairs):
    # Extracted the two columns for this pair
    col1, col2 = pair
    data = df[[col1, col2]]
    
    # Performed KMeans clustering with 4 clusters
    kmeans_4 = KMeans(n_clusters=4, init='k-means++', max_iter=100, random_state=42)
    kmeans_4.fit(data)
    labels_k4 = kmeans_4.labels_

    # Calculated silhouette score and add to list of scores
    score_k4 = silhouette_score(data, labels_k4)
    silhouette_scores_k4.append(score_k4)
    
    # Visualized the clustering results on the subplot matrix
    ax[i].scatter(data[col1], data[col2], c=labels_k4, cmap='viridis')
    ax[i].set_xlabel(col1)
    ax[i].set_ylabel(col2)
    ax[i].set_title(f'KMeans Clustering (4 clusters) - {col1} vs. {col2}')

# Calculated average silhouette score
avg_silhouette_score_kmeans4 = sum(silhouette_scores_k4) / len(silhouette_scores_k4)

# Removed any extra subplots from the subplot matrix
for j in range(i+1, nrows*ncols):
    fig.delaxes(ax[j])

# Adjusted the spacing between the subplots and show the figure
fig.tight_layout()
plt.show()

# Printed average silhouette score
print(avg_silhouette_score_kmeans4)

"""## Agglomerative Clustering

### Deciding on the number of clusters using the dendrogram method
"""

# Plotted the dendrogram
plt.figure(figsize=(12, 8))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
dend = shc.dendrogram(shc.linkage(df,method='ward'))

# Printed the optimal number of clusters based on the dendrogram
unique_colours = set(dend['color_list'])
optimal_number_of_clusters_agg = len(unique_colours) - 1
print(optimal_number_of_clusters_agg)

"""From the dendrogram, it is clear that the optimal number of clusters is 3 (with each cluster being displayed in a different colour).

### Visualizing the clusters
"""

# Created AgglomerativeClustering instance with 3 clusters and fit the model to the data
agg = AgglomerativeClustering(n_clusters=3)
agg_labels = agg.fit_predict(df)

# Generated all possible combinations of column pairs
column_pairs = itertools.combinations(df.columns, 2)

# Defined the number of rows and columns for the subplot matrix
nrows = 16
ncols = 5

# Created a figure and subplots with the defined number of rows and columns
fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 40))

# Reshaped the axis array to a flat array for easy indexing
ax = ax.flatten()

# Initialized list to store silhouette scores
silhouette_scores_agg = []

# Looped over each column pair and plot the agglomerative clustering results
for i, pair in enumerate(column_pairs):
    # Extracted the two columns for this pair
    col1, col2 = pair
    data = df[[col1, col2]]
    
    # Performed agglomerative clustering with 3 clusters
    agg_3 = AgglomerativeClustering(n_clusters=3)
    agg_3.fit(data)
    labels_agg = agg_3.labels_

    # Calculated silhouette score and add to list of scores
    score_agg = silhouette_score(data, labels_agg)
    silhouette_scores_agg.append(score_agg)
    
    # Visualized the clustering results on the subplot matrix
    ax[i].scatter(data[col1], data[col2], c=labels_agg, cmap='viridis')
    ax[i].set_xlabel(col1)
    ax[i].set_ylabel(col2)
    ax[i].set_title(f'Agglomerative Clustering (3 clusters) - {col1} vs. {col2}')

# Calculated average silhouette score
avg_silhouette_score_agg = sum(silhouette_scores_agg) / len(silhouette_scores_agg)

# Removed any extra subplots from the subplot matrix
for j in range(i+1, nrows*ncols):
    fig.delaxes(ax[j])

# Adjusted the spacing between the subplots and show the figure
fig.tight_layout()
plt.show()

# Printed average silhouette score
print(avg_silhouette_score_agg)

"""## DBSCAN

In using DBSCAN, I chose the range of hyperparameters to search based on trial and error.
"""

# Defined the range of hyperparameters to search
eps_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]
min_samples_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# Looped over all hyperparameter combinations and calculate silhouette score
best_score = -1
best_eps = None
best_min_samples = None
for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(df)
        if len(set(labels)) > 1:
            score = silhouette_score(df, labels)
            if score > best_score:
                best_score = score
                best_eps = eps
                best_min_samples = min_samples

print("Best eps:", best_eps)
print("Best min_samples:", best_min_samples)

# Fit DBSCAN with the best hyperparameters on the original dataset
best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)
best_dbscan_labels = best_dbscan.fit_predict(df)

# Computed and displayed silhouette score
silhouette_dbscan = silhouette_score(df, best_dbscan_labels)
print("Silhouette score:", silhouette_dbscan)

# Counted and displayed the number of data points in each cluster
unique_labels, counts = np.unique(best_dbscan_labels, return_counts=True)
for label, count in zip(unique_labels, counts):
    print(f"Cluster {label}: {count} data points")

# Generated all possible combinations of column pairs
column_pairs = itertools.combinations(df.columns, 2)

# Performed DBSCAN clustering on each pair of columns and visualize the results
fig, axes = plt.subplots(13, 6, figsize=(20, 40))
axes = axes.flatten()
for i, pair in enumerate(column_pairs):
    # Extracted the two columns for this pair
    col1, col2 = pair
    data = df[[col1, col2]]
    
    # Performed DBSCAN clustering with eps=best_eps and min_samples=best_min_samples
    dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)
    labels = dbscan.fit_predict(data)
    
    # Get the number of clusters and create a custom color map
    n_clusters = len(set(labels))
    
    # Visualize the clustering results
    axes[i].scatter(data[col1], data[col2], c=labels, cmap='viridis')
    axes[i].set_xlabel(col1)
    axes[i].set_ylabel(col2)
    axes[i].set_title(f'DBSCAN Clustering - {col1} vs. {col2}')
    
# Hide unused subplots
for j in range(i+1, len(axes)):
    axes[j].axis('off')
    
plt.tight_layout()
plt.show()

"""# Comparing the Clustering Results

Based on a visual inspection of the three clustering results from kmeans and/or agglomerative clustering, none of them appeared to have very well-separated clusters. Further, there was a wide range of cluster sizes for all of the clustering results using kmeans and agglomerative clustering. These observations may suggest that neither of the chosen algorithms were best-suited to the data. 

Unfortunately, due to time constraints, I was unable to get my DBSCAN visualizations to display the clusters as different colour so I could not inspect the plots individually. 

That said, when comparing the silhouette scores of all the clustering results, we found that using k-means clustering with three clusters resulted in the silhouette score closest to 1. This would suggest that using k-means clustering with three clusters yielded a better clustering result than both k-means clustering with four clusters, agglomerative clustering with three clusters and the DBSCAN with eps = 6, min_samples = 1. 

As such, I opted to use k-means clustering with three clusters to find outliers and when generate the one column CSV.

# Finding and Visualizing Outliers
"""

# Generated all possible combinations of column pairs
column_pairs = itertools.combinations(df.columns, 2)

# Define the number of rows and columns for the subplot matrix
nrows = 16
ncols = 5

# Created a figure and subplots with the defined number of rows and columns
fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 40))

# Reshaped the axis array to a flat array for easy indexing
ax = ax.flatten()

# Initialized list to store silhouette scores
silhouette_scores_k3 = []

# Looped over each column pair and plot the KMeans clustering results
for i, pair in enumerate(column_pairs):
    # Extract the two columns for this pair
    col1, col2 = pair
    data = df[[col1, col2]]
    
    # Performed KMeans clustering with 3 clusters
    kmeans_3 = KMeans(n_clusters=3, init='k-means++', max_iter=100, random_state=42)
    kmeans_3.fit(data)
    labels_k3 = kmeans_3.labels_

    # Calculated silhouette score and add to list of scores
    score_k3 = silhouette_score(data, labels_k3)
    silhouette_scores_k3.append(score_k3)
    
    # Visualized the clustering results on the subplot matrix
    ax[i].scatter(data[labels_k3!=-1][col1], data[labels_k3!=-1][col2], c=labels_k3[labels_k3!=-1], cmap='viridis', marker='o', label='Inliers')
    ax[i].scatter(data[labels_k3==-1][col1], data[labels_k3==-1][col2], c='red', marker='x', label='Outliers')
    ax[i].set_xlabel(col1)
    ax[i].set_ylabel(col2)
    ax[i].set_title(f'KMeans Clustering (3 clusters) - {col1} vs. {col2}')
    ax[i].legend()

# Calculated average silhouette score
avg_silhouette_score_kmeans3 = sum(silhouette_scores_k3) / len(silhouette_scores_k3)

# Removed any extra subplots from the subplot matrix
for j in range(i+1, nrows*ncols):
    fig.delaxes(ax[j])

# Adjusted the spacing between the subplots and show the figure
fig.tight_layout()
plt.show()

"""# Generating one column CSV


"""

# Determined the number of clusters and outlier label
num_clusters = len(set(labels_k3)) - (1 if -1 in labels_k3 else 0)
outlier_label = np.max(labels_k3) + 1

# Assigned outliers to their own cluster
labels_k3[labels_k3 == -1] = outlier_label

# Saved the cluster labels to a one column CSV file
cluster_labels = pd.DataFrame(labels_k3, columns=['cluster'])
cluster_labels.to_csv('cluster_labels.csv', index=False)

# Counted the number of entries in each cluster label
counts = cluster_labels['cluster'].value_counts()

# Created a dataframe with cluster label and numbered of entries columns
cluster_counts = pd.DataFrame({'cluster': counts.index, 'count': counts.values})

# Sorted the dataframe by cluster label
cluster_counts = cluster_counts.sort_values(by=['cluster']).reset_index(drop=True)

# Printed the dataframe
print(cluster_counts)

"""# Archive"""

# Generated all possible combinations of column pairs
column_pairs = itertools.combinations(df.columns, 2)

# Performed k-means clustering on each pair of columns and visualize the results
for pair in column_pairs:
    # Extracted the two columns for this pair
    col1, col2 = pair
    data = df[[col1, col2]]
    
    # Performed k-means clustering with 3 clusters
    kmeans_3 = KMeans(n_clusters=3, init='k-means++', max_iter=100, random_state=42)
    kmeans_3.fit(data)
    labels = kmeans_3.labels_
    
    # Visualized the clustering results
    plt.scatter(data[col1], data[col2], c=labels, cmap='viridis')
    plt.xlabel(col1)
    plt.ylabel(col2)
    plt.title(f'KMeans Clustering (3 clusters) - {col1} vs. {col2}')
    plt.show()

# Generated all possible combinations of column pairs
column_pairs = itertools.combinations(df.columns, 2)

# Performed k-means clustering on each pair of columns and visualize the results
for pair in column_pairs:
    # Extracted the two columns for this pair
    col1, col2 = pair
    data = df[[col1, col2]]
    
    # Performed k-means clustering with 4 clusters
    kmeans_4 = KMeans(n_clusters=4, init='k-means++', max_iter=100, random_state=42)
    kmeans_4.fit(data)
    labels = kmeans_4.labels_
    
    # Visualized the clustering results
    plt.scatter(data[col1], data[col2], c=labels, cmap='viridis')
    plt.xlabel(col1)
    plt.ylabel(col2)
    plt.title(f'KMeans Clustering (4 clusters) - {col1} vs. {col2}')
    plt.show()

# Generated all possible combinations of column pairs
column_pairs = itertools.combinations(df.columns, 2)

# Performed agglomerative clustering on each pair of columns and visualize the results
for pair in column_pairs:
    # Extracted the two columns for this pair
    col1, col2 = pair
    data = df[[col1, col2]]
    
    # Performed agglomerative clustering with 3 clusters
    agglomerative_3 = AgglomerativeClustering(n_clusters=3, linkage='ward')
    labels = agglomerative_3.fit_predict(data)
    
    # Visualized the clustering results
    plt.scatter(data[col1], data[col2], c=labels, cmap='viridis')
    plt.xlabel(col1)
    plt.ylabel(col2)
    plt.title(f'Agglomerative Clustering (3 clusters) - {col1} vs. {col2}')
    plt.show()

"""Based on a visual inspection of the three clustering results above, none of them appeared to have very well-separated clusters. Further, there was a wide range of cluster sizes for all three of the clustering results. These observations may suggest that none of the chosen algorithms were best-suited to the data. That said, due to time constraints, I was unable to try other clustering algorithms (such as DBSCAN and tSNE). 

On a separate note, when comparing the silhouette scores of the three clustering results, we found that using k-means clustering with four clusters resulted in the silhouette score closest to 1. This would suggest that using k-means clustering with four clusters yielded a better clustering result than both k-means clustering with three clusters and agglomerative clustering with three clusters. 

As such, I opted to use k-means clustering with four clusters to find outliers and when generate the one column CSV. 
"""